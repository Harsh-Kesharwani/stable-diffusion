{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d50f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# check if the model is downloaded,  if not download it\n",
    "import os\n",
    "if not os.path.exists(\"instruct-pix2pix-00-22000.ckpt\"):\n",
    "    !wget https://huggingface.co/timbrooks/instruct-pix2pix/resolve/main/instruct-pix2pix-00-22000.ckpt\n",
    "else:\n",
    "    print(\"Model already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3598a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded finetuned weights from maskfree_finetuned_weights.safetensors\n",
      "Loading 0.in_proj.weight\n",
      "Loading 0.out_proj.weight\n",
      "Loading 0.out_proj.bias\n",
      "Loading 8.in_proj.weight\n",
      "Loading 8.out_proj.weight\n",
      "Loading 8.out_proj.bias\n",
      "Loading 16.in_proj.weight\n",
      "Loading 16.out_proj.weight\n",
      "Loading 16.out_proj.bias\n",
      "Loading 24.in_proj.weight\n",
      "Loading 24.out_proj.weight\n",
      "Loading 24.out_proj.bias\n",
      "Loading 32.in_proj.weight\n",
      "Loading 32.out_proj.weight\n",
      "Loading 32.out_proj.bias\n",
      "Loading 40.in_proj.weight\n",
      "Loading 40.out_proj.weight\n",
      "Loading 40.out_proj.bias\n",
      "Loading 48.in_proj.weight\n",
      "Loading 48.out_proj.weight\n",
      "Loading 48.out_proj.bias\n",
      "Loading 56.in_proj.weight\n",
      "Loading 56.out_proj.weight\n",
      "Loading 56.out_proj.bias\n",
      "Loading 64.in_proj.weight\n",
      "Loading 64.out_proj.weight\n",
      "Loading 64.out_proj.bias\n",
      "Loading 72.in_proj.weight\n",
      "Loading 72.out_proj.weight\n",
      "Loading 72.out_proj.bias\n",
      "Loading 80.in_proj.weight\n",
      "Loading 80.out_proj.weight\n",
      "Loading 80.out_proj.bias\n",
      "Loading 88.in_proj.weight\n",
      "Loading 88.out_proj.weight\n",
      "Loading 88.out_proj.bias\n",
      "Loading 96.in_proj.weight\n",
      "Loading 96.out_proj.weight\n",
      "Loading 96.out_proj.bias\n",
      "Loading 104.in_proj.weight\n",
      "Loading 104.out_proj.weight\n",
      "Loading 104.out_proj.bias\n",
      "Loading 112.in_proj.weight\n",
      "Loading 112.out_proj.weight\n",
      "Loading 112.out_proj.bias\n",
      "Loading 120.in_proj.weight\n",
      "Loading 120.out_proj.weight\n",
      "Loading 120.out_proj.bias\n",
      "\n",
      "Attention module weights loaded from {finetune_weights_path} successfully.\n"
     ]
    }
   ],
   "source": [
    "import load_model\n",
    "\n",
    "models=load_model.preload_models_from_standard_weights(ckpt_path=\"instruct-pix2pix-00-22000.ckpt\", device=\"cuda\", finetune_weights_path=\"maskfree_finetuned_weights.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5627b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahesh/miniconda3/envs/harsh/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset vitonhd loaded, total 20 pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.26it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.93it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.84it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.25it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.34it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.26it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  7.03it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.18it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.99it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.27it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.18it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.19it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.99it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  7.08it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.35it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.38it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.92it/s]\n",
      "100%|██████████| 50/50 [00:06<00:00,  7.35it/s]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from VITON_Dataset import VITONHDTestDataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from utils import to_pil_image\n",
    "from CatVTON_model import CatVTONPix2PixPipeline\n",
    "\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    args=argparse.Namespace()\n",
    "    args.__dict__= {\n",
    "        \"dataset_name\": \"vitonhd\",\n",
    "        \"data_root_path\": \"./sample_dataset\",\n",
    "        \"output_dir\": \"./mask-free-output\",\n",
    "        \"seed\": 555,\n",
    "        \"batch_size\": 1,\n",
    "        \"num_inference_steps\": 50,\n",
    "        \"guidance_scale\": 2.5,\n",
    "        \"width\": 384,\n",
    "        \"height\": 512,\n",
    "        \"eval_pair\": False,\n",
    "        \"concat_eval_results\": True,\n",
    "        \"allow_tf32\": True,\n",
    "        \"dataloader_num_workers\": 4,\n",
    "        \"mixed_precision\": 'no',\n",
    "        \"concat_axis\": 'y',\n",
    "        \"enable_condition_noise\": True,\n",
    "        \"is_train\": False\n",
    "    }\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = CatVTONPix2PixPipeline(\n",
    "        weight_dtype={\n",
    "            \"no\": torch.float32,\n",
    "            \"fp16\": torch.float16,\n",
    "            \"bf16\": torch.bfloat16,\n",
    "        }[args.mixed_precision],\n",
    "        device=\"cuda\",\n",
    "        skip_safety_check=True,\n",
    "        models=models,\n",
    "    )\n",
    "    # Dataset\n",
    "    if args.dataset_name == \"vitonhd\":\n",
    "        dataset = VITONHDTestDataset(args)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataset name {args.dataset}.\")\n",
    "    print(f\"Dataset {args.dataset_name} loaded, total {len(dataset)} pairs.\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.dataloader_num_workers\n",
    "    )\n",
    "        \n",
    "    # Inference\n",
    "    generator = torch.Generator(device='cuda').manual_seed(args.seed)\n",
    "    args.output_dir = os.path.join(args.output_dir, f\"{args.dataset_name}-{args.height}\", \"paired\" if args.eval_pair else \"unpaired\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "        \n",
    "    for batch in tqdm(dataloader):\n",
    "        person_images = batch['person']\n",
    "        cloth_images = batch['cloth']\n",
    "\n",
    "        results = pipeline(\n",
    "            person_images,\n",
    "            cloth_images,\n",
    "            num_inference_steps=args.num_inference_steps,\n",
    "            guidance_scale=args.guidance_scale,\n",
    "            height=args.height,\n",
    "            width=args.width,\n",
    "            generator=generator,\n",
    "        )\n",
    "        \n",
    "        if args.concat_eval_results:\n",
    "            person_images = to_pil_image(person_images)\n",
    "            cloth_images = to_pil_image(cloth_images)\n",
    "        for i, result in enumerate(results):\n",
    "            person_name = batch['person_name'][i]\n",
    "            output_path = os.path.join(args.output_dir, person_name)\n",
    "            if not os.path.exists(os.path.dirname(output_path)):\n",
    "                os.makedirs(os.path.dirname(output_path))\n",
    "            if args.concat_eval_results:\n",
    "                w, h = result.size\n",
    "                concated_result = Image.new('RGB', (w*3, h))\n",
    "                concated_result.paste(person_images[i], (0, 0))\n",
    "                concated_result.paste(cloth_images[i], (w, 0))  \n",
    "                concated_result.paste(result, (w*2, 0))\n",
    "                result = concated_result\n",
    "            result.save(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39537851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb6113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c374cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddce5df",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harsh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
