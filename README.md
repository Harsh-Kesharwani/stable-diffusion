# üé® Stable Diffusion from Scratch

<div align="center">

![Stable Diffusion](https://img.shields.io/badge/Stable%20Diffusion-From%20Scratch-blue?style=for-the-badge&logo=pytorch)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.10+-green?style=for-the-badge&logo=python&logoColor=white)

*A complete implementation of Stable Diffusion built from scratch using PyTorch*

</div>

## Overview

This repository contains a **from-scratch implementation** of Stable Diffusion using PyTorch. All core components including VAE, CLIP, U-Net, and DDPM sampling are implemented without relying on external diffusion libraries.

## Project Structure

```
stable-diffusion/
‚îú‚îÄ‚îÄ attention.py          # Self-attention and cross-attention mechanisms
‚îú‚îÄ‚îÄ clip.py               # CLIP text encoder implementation
‚îú‚îÄ‚îÄ ddpm.py               # DDPM sampling scheduler
‚îú‚îÄ‚îÄ decoder.py            # VAE decoder
‚îú‚îÄ‚îÄ encoder.py            # VAE encoder  
‚îú‚îÄ‚îÄ diffusion.py          # U-Net diffusion model
‚îú‚îÄ‚îÄ model.py              # Model loading utilities
‚îú‚îÄ‚îÄ pipeline.py           # Main diffusion pipeline
‚îú‚îÄ‚îÄ interface.py          # Interactive generation script
‚îú‚îÄ‚îÄ test.ipynb            # Jupyter notebook for testing
‚îú‚îÄ‚îÄ vocab.json            # CLIP tokenizer vocabulary
‚îú‚îÄ‚îÄ merges.txt            # CLIP tokenizer merges
‚îî‚îÄ‚îÄ requirements.txt      # Python dependencies
```

## Setup

### 1. Clone Repository
```bash
git clone https://github.com/Harsh-Kesharwani/stable-diffusion.git
cd stable-diffusion
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Download Model Weights
```bash
# Download Inkpunk Diffusion model
wget -O inkpunk-diffusion-v1.ckpt "https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/inkpunk-diffusion-v1.ckpt?download=true"
```

### 4. Run Inference
```bash
python interface.py
```

Or use the Jupyter notebook:
```bash
jupyter notebook test.ipynb
```

## Features

- ‚úÖ **Complete from-scratch implementation**
- ‚úÖ **VAE Encoder/Decoder** for latent space conversion
- ‚úÖ **CLIP Text Encoder** for text conditioning
- ‚úÖ **U-Net Diffusion Model** with attention mechanisms
- ‚úÖ **DDPM Sampling** scheduler
- ‚úÖ **Text-to-image generation**
- ‚úÖ **Custom attention layers** (self-attention & cross-attention)

## Usage

The `interface.py` script provides an interactive way to generate images:

```python
# Example prompt
prompt = "a beautiful landscape, digital art, trending on artstation"
```

## Model Details

- **Base Model**: Inkpunk Diffusion v1 (fine-tuned Stable Diffusion)
- **Resolution**: 512x512
- **Scheduler**: DDPM with 1000 timesteps
- **Text Encoder**: CLIP ViT-L/14

## Requirements

- Python 3.10+
- PyTorch 1.12+
- CUDA-compatible GPU (recommended)
- 8GB+ GPU memory

## Author

**Harsh Kesharwani**

[![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/Harsh-Kesharwani)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/harsh-kesharwani/)

## License

MIT License - see [LICENSE](LICENSE) for details.

---

<div align="center">

**‚≠ê Star this repo if you found it helpful!**

*Built with ‚ù§Ô∏è for learning and understanding diffusion models*

</div>